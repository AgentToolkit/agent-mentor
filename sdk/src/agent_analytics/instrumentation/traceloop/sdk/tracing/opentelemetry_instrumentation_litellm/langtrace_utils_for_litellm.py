# Utility variables and functions adapted from Langtrace to support LiteLLM instrumentation,
# eliminating the direct dependency on the Langtrace package.
# Adapted from langtrace-python-sdk version: 3.3.4

"""
Copyright (c) 2024 Scale3 Labs
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""
from __future__ import annotations
from pydantic import BaseModel, ConfigDict, Field
import logging
import json
from typing import Dict, Optional, TypedDict, Union
from typing import Any, Callable, List, Tuple, Literal, TypeVar, cast
import os

from opentelemetry import baggage
from opentelemetry.trace import Span
from opentelemetry.trace.status import StatusCode
from tiktoken import get_encoding, list_encoding_names

LANGTRACE_SDK_VERSION = "3.3.4"


# From: langtrace/trace_attributes/models/llm_span_attributes.py
# generated by datamodel-codegen:
#   filename:  llm_span_attributes.json
#   timestamp: 2024-07-24T15:56:12+00:00
class LLMSpanAttributes(BaseModel):
    model_config = ConfigDict(extra="allow")

    langtrace_service_name: str = Field(
        ...,
        alias="langtrace.service.name",
        description="Name of the service. Includes all supported service providers by langtrace",
    )
    langtrace_service_type: str = Field(
        ...,
        alias="langtrace.service.type",
        description="Type of the service. Allowed values: [llm, vectordb, framework]",
    )
    langtrace_service_version: Optional[str] = Field(
        None,
        alias="langtrace.service.version",
        description="Version of the service provider client",
    )
    langtrace_version: str = Field(..., alias="langtrace.version")
    langtrace_sdk_name: str = Field(..., alias="langtrace.sdk.name")
    url_full: str = Field(..., alias="url.full", description="Full URL of the request")
    url_path: str = Field(..., alias="url.path", description="Path of the request")
    gen_ai_operation_name: str = Field(
        ...,
        alias="gen_ai.operation.name",
        description="The name of the operation being performed.",
    )
    gen_ai_request_model: str = Field(
        ...,
        alias="gen_ai.request.model",
        description="Model name from the input request",
    )
    gen_ai_response_model: Optional[str] = Field(
        None, alias="gen_ai.response.model", description="Model name from the response"
    )
    gen_ai_request_temperature: Optional[float] = Field(
        None,
        alias="gen_ai.request.temperature",
        description="Temperature value from the input request",
    )
    gen_ai_request_logit_bias: Optional[str] = Field(
        None,
        alias="gen_ai.request.logit_bias",
        description="Likelihood bias of the specified tokens the input request.",
    )
    gen_ai_request_logprobs: Optional[bool] = Field(
        None,
        alias="gen_ai.request.logprobs",
        description="Logprobs flag returns log probabilities.",
    )
    gen_ai_request_top_logprobs: Optional[float] = Field(
        None,
        alias="gen_ai.request.top_logprobs",
        description="Integer between 0 and 5 specifying the number of most likely tokens to return.",
    )
    gen_ai_request_top_p: Optional[float] = Field(
        None,
        alias="gen_ai.request.top_p",
        description="Top P value from the input request",
    )
    gen_ai_request_top_k: Optional[float] = Field(
        None,
        alias="gen_ai.request.top_k",
        description="Top K results to return from the input request",
    )
    gen_ai_user: Optional[str] = Field(
        None, alias="gen_ai.user", description="User ID from the input request"
    )
    gen_ai_prompt: Optional[str] = Field(
        None, alias="gen_ai.prompt", description="Prompt text from the input request"
    )
    gen_ai_completion: Optional[str] = Field(
        None,
        alias="gen_ai.completion",
        description='Completion text from the response. This will be an array of json objects with the following format {"role": "", "content": ""}. Role can be one of the following values: [system, user, assistant, tool]',  # noqa: E501
    )
    gen_ai_request_stream: Optional[bool] = Field(
        None,
        alias="gen_ai.request.stream",
        description="Stream flag from the input request",
    )
    gen_ai_request_encoding_formats: Optional[List[str]] = Field(
        None,
        alias="gen_ai.request.encoding_formats",
        description="Encoding formats from the input request. Allowed values: ['float', 'int8','uint8', 'binary', 'ubinary', 'base64']",  # noqa: E501
    )
    gen_ai_completion_chunk: Optional[str] = Field(
        None,
        alias="gen_ai.completion.chunk",
        description="Chunk text from the response",
    )
    gen_ai_request_dimensions: Optional[float] = Field(
        None,
        alias="gen_ai.request.dimensions",
        description="Dimensions from the input request",
    )
    gen_ai_response_id: Optional[str] = Field(
        None,
        alias="gen_ai.response_id",
        description="Response ID from the output response",
    )
    gen_ai_response_finish_reasons: Optional[List[str]] = Field(
        None,
        alias="gen_ai.response.finish_reasons",
        description="Array of reasons the model stopped generating tokens, corresponding to each generation received",
    )
    gen_ai_system_fingerprint: Optional[str] = Field(
        None,
        alias="gen_ai.system_fingerprint",
        description="System fingerprint of the system that generated the response",
    )
    gen_ai_request_documents: Optional[str] = Field(
        None,
        alias="gen_ai.request.documents",
        description="Array of documents from the input request json stringified",
    )
    gen_ai_request_is_search_required: Optional[bool] = Field(
        None,
        alias="gen_ai.request.is_search_required",
        description="Search flag from the input request",
    )
    gen_ai_request_tool_choice: Optional[str] = Field(
        None,
        alias="gen_ai.request.tool_choice",
        description="Tool choice from the input request",
    )
    gen_ai_response_tool_calls: Optional[str] = Field(
        None,
        alias="gen_ai.response.tool_calls",
        description="Array of tool calls from the response json stringified",
    )
    gen_ai_request_max_tokens: Optional[float] = Field(
        None,
        alias="gen_ai.request.max_tokens",
        description="The maximum number of tokens the LLM generates for a request.",
    )
    gen_ai_usage_input_tokens: Optional[float] = Field(
        None,
        alias="gen_ai.usage.input_tokens",
        description="The number of tokens used in the llm prompt.",
    )
    gen_ai_usage_total_tokens: Optional[float] = Field(
        None,
        alias="gen_ai.usage.total_tokens",
        description="The total number of tokens used in the llm request.",
    )
    gen_ai_usage_output_tokens: Optional[float] = Field(
        None,
        alias="gen_ai.usage.output_tokens",
        description="The number of tokens in the llm response.",
    )
    gen_ai_usage_search_units: Optional[float] = Field(
        None,
        alias="gen_ai.usage.search_units",
        description="The number of search units used in the request.",
    )
    gen_ai_request_seed: Optional[str] = Field(
        None, alias="gen_ai.request.seed", description="Seed from the input request"
    )
    gen_ai_request_frequency_penalty: Optional[float] = Field(
        None,
        alias="gen_ai.request.frequency_penalty",
        description="Frequency penalty from the input request",
    )
    gen_ai_request_presence_penalty: Optional[float] = Field(
        None,
        alias="gen_ai.request.presence_penalty",
        description="Presence penalty from the input request",
    )
    gen_ai_request_connectors: Optional[str] = Field(
        None,
        alias="gen_ai.request.connectors",
        description="An array of connectors from the input request json stringified",
    )
    gen_ai_request_tools: Optional[str] = Field(
        None,
        alias="gen_ai.request.tools",
        description="An array of tools from the input request json stringified",
    )
    gen_ai_request_tool_results: Optional[str] = Field(
        None,
        alias="gen_ai.request.tool_results",
        description="An array of tool results from the input request json stringified",
    )
    gen_ai_request_embedding_inputs: Optional[str] = Field(
        None,
        alias="gen_ai.request.embedding_inputs",
        description="An array of embedding inputs from the input request json stringified",
    )
    gen_ai_request_embedding_dataset_id: Optional[str] = Field(
        None,
        alias="gen_ai.request.embedding_dataset_id",
        description="Embedding dataset ID from the input request",
    )
    gen_ai_request_embedding_input_type: Optional[str] = Field(
        None,
        alias="gen_ai.request.embedding_input_type",
        description="Embedding input type from the input request. Allowed values: [ 'search_document', 'search_query', 'classification', 'clustering']",  # noqa: E501
    )
    gen_ai_request_embedding_job_name: Optional[str] = Field(
        None,
        alias="gen_ai.request.embedding_job_name",
        description="Embedding job name from the input request",
    )
    gen_ai_image_size: Optional[str] = Field(
        None,
        alias="gen_ai.image.size",
        description="Image size from the input request. Allowed values: ['256x256', '512x512', '1024x1024']",
    )
    gen_ai_request_response_format: Optional[str] = Field(
        None,
        alias="gen_ai.request.response_format",
        description="Response format from the input request. Allowed values: ['url', 'b64_json']",
    )
    http_max_retries: Optional[int] = Field(None, alias="http.max.retries")
    http_timeout: Optional[int] = Field(None, alias="http.timeout")
    gen_ai_cohere_rerank_query: Optional[str] = Field(
        None,
        alias="gen_ai.cohere.rerank.query",
        description="Query from the input request for the rerank api",
    )
    gen_ai_cohere_rerank_results: Optional[str] = Field(
        None,
        alias="gen_ai.cohere.rerank.results",
        description="Results from the rerank api",
    )


# From: langtrace/trace_attributes/__init__.py
class SpanAttributes:
    LLM_SYSTEM = "gen_ai.system"
    LLM_OPERATION_NAME = "gen_ai.operation.name"
    LLM_REQUEST_MODEL = "gen_ai.request.model"
    LLM_REQUEST_MAX_TOKENS = "gen_ai.request.max_tokens"
    LLM_REQUEST_TEMPERATURE = "gen_ai.request.temperature"
    LLM_REQUEST_TOP_P = "gen_ai.request.top_p"
    LLM_SYSTEM_FINGERPRINT = "gen_ai.system_fingerprint"

    LLM_REQUEST_DOCUMENTS = "gen_ai.request.documents"
    LLM_REQUEST_SEARCH_REQUIRED = "gen_ai.request.is_search_required"
    LLM_PROMPTS = "gen_ai.prompt"
    LLM_CONTENT_PROMPT = "gen_ai.content.prompt"
    LLM_COMPLETIONS = "gen_ai.completion"
    LLM_CONTENT_COMPLETION = "gen_ai.content.completion"

    LLM_RESPONSE_MODEL = "gen_ai.response.model"
    LLM_USAGE_COMPLETION_TOKENS = "gen_ai.usage.output_tokens"
    LLM_USAGE_PROMPT_TOKENS = "gen_ai.usage.input_tokens"
    LLM_USAGE_TOTAL_TOKENS = "gen_ai.usage.total_tokens"
    LLM_USAGE_TOKEN_TYPE = "gen_ai.usage.token_type"
    LLM_USAGE_SEARCH_UNITS = "gen_ai.usage.search_units"
    LLM_GENERATION_ID = "gen_ai.generation_id"
    LLM_TOKEN_TYPE = "gen_ai.token.type"
    LLM_RESPONSE_ID = "gen_ai.response_id"
    LLM_URL = "url.full"
    LLM_PATH = "url.path"
    LLM_RESPONSE_FORMAT = "gen_ai.request.response_format"
    LLM_IMAGE_SIZE = "gen_ai.image.size"
    LLM_REQUEST_ENCODING_FORMATS = "gen_ai.request.encoding_formats"
    LLM_REQUEST_DIMENSIONS = "gen_ai.request.dimensions"
    LLM_REQUEST_SEED = "gen_ai.request.seed"
    LLM_REQUEST_TOP_LOGPROPS = "gen_ai.request.top_props"
    LLM_REQUEST_LOGPROPS = "gen_ai.request.log_props"
    LLM_REQUEST_LOGITBIAS = "gen_ai.request.logit_bias"

    LLM_REQUEST_TYPE = "gen_ai.request.type"
    LLM_HEADERS = "gen_ai.headers"

    LLM_USER = "gen_ai.user"
    LLM_TOOLS = "gen_ai.request.tools"
    LLM_TOOL_CHOICE = "gen_ai.request.tool_choice"
    LLM_TOOL_RESULTS = "gen_ai.request.tool_results"

    LLM_TOP_K = "gen_ai.request.top_k"
    LLM_IS_STREAMING = "gen_ai.request.stream"
    LLM_FREQUENCY_PENALTY = "gen_ai.request.frequency_penalty"
    LLM_PRESENCE_PENALTY = "gen_ai.request.presence_penalty"
    LLM_CHAT_STOP_SEQUENCES = "gen_ai.chat.stop_sequences"
    LLM_REQUEST_FUNCTIONS = "gen_ai.request.functions"
    LLM_REQUEST_REPETITION_PENALTY = "gen_ai.request.repetition_penalty"
    LLM_RESPONSE_FINISH_REASON = "gen_ai.response.finish_reasons"
    LLM_RESPONSE_STOP_REASON = "gen_ai.response.stop_reason"
    LLM_CONTENT_COMPLETION_CHUNK = "gen_ai.completion.chunk"
    # embeddings
    LLM_REQUEST_EMBEDDING_INPUTS = "gen_ai.request.embedding_inputs"
    LLM_REQUEST_EMBEDDING_DATASET_ID = "gen_ai_request_embedding_dataset_id"
    LLM_REQUEST_EMBEDDING_INPUT_TYPE = "gen_ai.request.embedding_input_type"
    LLM_REQUEST_EMBEDDING_JOB_NAME = "gen_ai.request.embedding_job_name"

    # Cohere
    LLM_COHERE_RERANK_QUERY = "gen_ai.cohere.rerank.query"
    LLM_COHERE_RERANK_RESULTS = "gen_ai.cohere.rerank.results"

    # Langtrace
    LANGTRACE_SDK_NAME = "langtrace.sdk.name"
    LANGTRACE_SERVICE_NAME = "langtrace.service.name"
    LANGTRACE_SERVICE_TYPE = "langtrace.service.type"
    LANGTRACE_SERVICE_VERSION = "langtrace.service.version"
    LANGTRACE_VERSION = "langtrace.version"

    # Http
    HTTP_MAX_RETRIES = "http.max.retries"
    HTTP_TIMEOUT = "http.timeout"


# From: langtrace_python_sdk/utils/silently_fail.py
F = TypeVar('F', bound=Callable[..., Any])


def silently_fail(func: F) -> F:
    """
    A decorator that catches exceptions thrown by the decorated function and logs them as warnings.
    """

    logger = logging.getLogger(func.__module__)

    def wrapper(*args: Tuple[Any, ...], **kwargs: dict[str, Any]) -> Any:
        try:
            return func(*args, **kwargs)
        except Exception as exception:
            logger.warning(
                "Failed to execute %s, error: %s", func.__name__, str(exception)
            )

    return cast(F, wrapper)


# From: langtrace_python_sdk/constants/instrumentation/common.py
TIKTOKEN_MODEL_MAPPING = {
    "gpt-4": "cl100k_base",
    "gpt-4-32k": "cl100k_base",
    "gpt-4-0125-preview": "cl100k_base",
    "gpt-4-1106-preview": "cl100k_base",
    "gpt-4-1106-vision-preview": "cl100k_base",
    "gpt-4o": "0200k_base",
    "gpt-4o-mini": "0200k_base",
}

SERVICE_PROVIDERS = {
    "ANTHROPIC": "Anthropic",
    "AZURE": "Azure",
    "CHROMA": "Chroma",
    "CREWAI": "CrewAI",
    "DSPY": "DSPy",
    "GROQ": "Groq",
    "LANGCHAIN": "Langchain",
    "LANGCHAIN_COMMUNITY": "Langchain Community",
    "LANGCHAIN_CORE": "Langchain Core",
    "LANGGRAPH": "Langgraph",
    "LITELLM": "Litellm",
    "LLAMAINDEX": "LlamaIndex",
    "OPENAI": "OpenAI",
    "PINECONE": "Pinecone",
    "COHERE": "Cohere",
    "PPLX": "Perplexity",
    "QDRANT": "Qdrant",
    "WEAVIATE": "Weaviate",
    "OLLAMA": "Ollama",
    "VERTEXAI": "VertexAI",
    "GEMINI": "Gemini",
    "MISTRAL": "Mistral",
    "EMBEDCHAIN": "Embedchain",
    "AUTOGEN": "Autogen",
    "XAI": "XAI",
    "MONGODB": "MongoDB",
    "AWS_BEDROCK": "AWS Bedrock",
    "CEREBRAS": "Cerebras",
}

LANGTRACE_ADDITIONAL_SPAN_ATTRIBUTES_KEY = "langtrace_additional_attributes"

# From: langtrace_python_sdk/constants/instrumentation/litellm.py
APIS = {
    "CHAT_COMPLETION": {
        "METHOD": "chat.completions.create",
        "ENDPOINT": "/chat/completions",
    },
    "IMAGES_GENERATION": {
        "METHOD": "images.generate",
        "ENDPOINT": "/images/generations",
    },
    "IMAGES_EDIT": {
        "METHOD": "images.edit",
        "ENDPOINT": "/images/edits",
    },
    "EMBEDDINGS_CREATE": {
        "METHOD": "embeddings.create",
        "ENDPOINT": "/embeddings",
    },
}

# From: langtrace_python_sdk/constants/__init__.py
LANGTRACE_SDK_NAME = "langtrace-python-sdk"


# From: langtrace_python_sdk/types/__init__.py
class NotGiven:
    """
    A sentinel singleton class used to distinguish omitted keyword arguments
    from those passed in with the value None (which may have different behavior).

    For example:

    ```py
    def get(timeout: Union[int, NotGiven, None] = NotGiven()) -> Response:
        ...


    get(timeout=1)  # 1s timeout
    get(timeout=None)  # No timeout
    get()  # Default timeout behavior, which may not be statically known at the method definition.
    ```
    """

    def __bool__(self) -> Literal[False]:
        return False

    def __repr__(self) -> str:
        return "NOT_GIVEN"


NOT_GIVEN = NotGiven()


# From: langtrace_python_sdk/utils/__init__.py
def set_span_attribute(span: Span, name, value):
    if value is not None:
        if value != "" or value != NOT_GIVEN:
            if name == SpanAttributes.LLM_PROMPTS:
                set_event_prompt(span, value)
            else:
                span.set_attribute(name, value)
    return


def set_event_prompt(span: Span, prompt):
    enabled = os.environ.get("TRACE_PROMPT_COMPLETION_DATA", "true")
    if enabled.lower() == "false":
        return

    span.add_event(
        name=SpanAttributes.LLM_CONTENT_PROMPT,
        attributes={
            SpanAttributes.LLM_PROMPTS: prompt,
        },
    )


# From: langtrace_python_sdk/utils/llm.py

def estimate_tokens(prompt):
    """
    Estimate the number of tokens in a prompt."""
    if prompt and len(prompt) > 0:
        # Simplified token estimation: count the words.
        return len([word for word in prompt.split() if word])
    return 0


def estimate_tokens_using_tiktoken(prompt, model):
    """
    Estimate the number of tokens in a prompt using tiktoken."""
    encoding = get_encoding(model)
    tokens = encoding.encode(prompt)
    return len(tokens)


def calculate_prompt_tokens(prompt_content, model):
    """
    Calculate the number of tokens in a prompt. If the model is supported by tiktoken, use it for the estimation.
    """
    try:
        tiktoken_model = TIKTOKEN_MODEL_MAPPING[model]
        return estimate_tokens_using_tiktoken(prompt_content, tiktoken_model)
    except Exception:
        return estimate_tokens(prompt_content)  # Fallback method


def get_base_url(instance):
    return (
        str(instance._client._base_url)
        if hasattr(instance, "_client") and hasattr(instance._client, "_base_url")
        else ""
    )


def get_extra_attributes() -> Union[Dict[str, Any], object]:
    extra_attributes = baggage.get_baggage(LANGTRACE_ADDITIONAL_SPAN_ATTRIBUTES_KEY)
    return extra_attributes or {}


def get_langtrace_attributes(version, service_provider, vendor_type="llm"):
    return {
        SpanAttributes.LANGTRACE_SDK_NAME: LANGTRACE_SDK_NAME,
        # SpanAttributes.LANGTRACE_VERSION: v(LANGTRACE_SDK_NAME),
        SpanAttributes.LANGTRACE_VERSION: LANGTRACE_SDK_VERSION,
        SpanAttributes.LANGTRACE_SERVICE_VERSION: version,
        SpanAttributes.LANGTRACE_SERVICE_NAME: service_provider,
        SpanAttributes.LANGTRACE_SERVICE_TYPE: vendor_type,
        SpanAttributes.LLM_SYSTEM: service_provider.lower(),
    }


def get_llm_request_attributes(kwargs, prompts=None, model=None, operation_name="chat"):

    user = kwargs.get("user", None)
    if prompts is None:
        prompts = (
            [{"role": user or "user", "content": kwargs.get("prompt")}]
            if "prompt" in kwargs
            else None
        )
    top_k = (
        kwargs.get("n", None)
        or kwargs.get("k", None)
        or kwargs.get("top_k", None)
        or kwargs.get("top_n", None)
    )

    top_p = kwargs.get("p", None) or kwargs.get("top_p", None)
    tools = kwargs.get("tools", None)
    tool_choice = kwargs.get("tool_choice", None)
    return {
        SpanAttributes.LLM_OPERATION_NAME: operation_name,
        SpanAttributes.LLM_REQUEST_MODEL: model
        or kwargs.get("model")
        or "gpt-3.5-turbo",
        SpanAttributes.LLM_IS_STREAMING: kwargs.get("stream"),
        SpanAttributes.LLM_REQUEST_TEMPERATURE: kwargs.get("temperature"),
        SpanAttributes.LLM_TOP_K: top_k,
        SpanAttributes.LLM_PROMPTS: json.dumps(prompts) if prompts else None,
        SpanAttributes.LLM_USER: user,
        SpanAttributes.LLM_REQUEST_TOP_P: top_p,
        SpanAttributes.LLM_REQUEST_MAX_TOKENS: kwargs.get("max_tokens"),
        SpanAttributes.LLM_SYSTEM_FINGERPRINT: kwargs.get("system_fingerprint"),
        SpanAttributes.LLM_PRESENCE_PENALTY: kwargs.get("presence_penalty"),
        SpanAttributes.LLM_FREQUENCY_PENALTY: kwargs.get("frequency_penalty"),
        SpanAttributes.LLM_REQUEST_SEED: kwargs.get("seed"),
        SpanAttributes.LLM_TOOLS: json.dumps(tools) if tools else None,
        SpanAttributes.LLM_TOOL_CHOICE: (
            json.dumps(tool_choice) if tool_choice else None
        ),
        SpanAttributes.LLM_REQUEST_LOGPROPS: kwargs.get("logprobs"),
        SpanAttributes.LLM_REQUEST_LOGITBIAS: kwargs.get("logit_bias"),
        SpanAttributes.LLM_REQUEST_TOP_LOGPROPS: kwargs.get("top_logprobs"),
    }


def get_span_name(operation_name):
    extra_attributes = get_extra_attributes()
    if extra_attributes is not None and "langtrace.span.name" in extra_attributes:
        return f'{operation_name}-{extra_attributes["langtrace.span.name"]}'
    return operation_name


def get_tool_calls(item):
    if isinstance(item, dict):
        if "tool_calls" in item and item["tool_calls"] is not None:
            return item["tool_calls"]
        return None

    else:
        if hasattr(item, "tool_calls") and item.tool_calls is not None:
            return item.tool_calls
        return None


def is_streaming(kwargs):
    return not (
        kwargs.get("stream") is False
        or kwargs.get("stream") is None
        or kwargs.get("stream") == NOT_GIVEN
    )


def set_event_completion(span: Span, result_content):
    enabled = os.environ.get("TRACE_PROMPT_COMPLETION_DATA", "true")
    if enabled.lower() == "false":
        return

    span.add_event(
        name=SpanAttributes.LLM_CONTENT_COMPLETION,
        attributes={
            SpanAttributes.LLM_COMPLETIONS: json.dumps(result_content),
        },
    )


class StreamWrapper:
    span: Span

    def __init__(
        self, stream, span, prompt_tokens=0, function_call=False, tool_calls=False
    ):
        self.stream = stream
        self.span = span
        self.prompt_tokens = prompt_tokens
        self.function_call = function_call
        self.tool_calls = tool_calls
        self.result_content = []
        self.completion_tokens = 0
        self._span_started = False
        self._response_model = None
        self.setup()

    def setup(self):
        if not self._span_started:
            self._span_started = True

    def cleanup(self):
        if self.completion_tokens == 0:
            response_model = "cl100k_base"
            if self._response_model in list_encoding_names():
                response_model = self._response_model
            self.completion_tokens = estimate_tokens_using_tiktoken(
                "".join(self.result_content), response_model
            )
        if self._span_started:
            set_span_attribute(
                self.span,
                SpanAttributes.LLM_RESPONSE_MODEL,
                self._response_model,
            )
            set_span_attribute(
                self.span,
                SpanAttributes.LLM_USAGE_PROMPT_TOKENS,
                self.prompt_tokens,
            )
            set_span_attribute(
                self.span,
                SpanAttributes.LLM_USAGE_COMPLETION_TOKENS,
                self.completion_tokens,
            )
            set_span_attribute(
                self.span,
                SpanAttributes.LLM_USAGE_TOTAL_TOKENS,
                self.prompt_tokens + self.completion_tokens,
            )
            set_event_completion(
                self.span,
                [
                    {
                        "role": "assistant",
                        "content": "".join(self.result_content),
                    }
                ],
            )
            self.span.set_status(StatusCode.OK)
            self.span.end()
            self._span_started = False

    def __enter__(self):
        self.setup()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

    async def __aenter__(self):
        self.setup()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

    def __iter__(self):
        return self

    def __next__(self):
        try:
            chunk = next(self.stream)
            self.process_chunk(chunk)
            return chunk
        except StopIteration:
            self.cleanup()
            raise

    def __aiter__(self):
        return self

    async def __anext__(self):
        try:
            chunk = await self.stream.__anext__()
            self.process_chunk(chunk)
            return chunk
        except StopAsyncIteration:
            self.cleanup()
            raise StopAsyncIteration

    def set_response_model(self, chunk):
        if self._response_model:
            return

        # OpenAI response model is set on all chunks
        if hasattr(chunk, "model") and chunk.model is not None:
            self._response_model = chunk.model

        # Anthropic response model is set on the first chunk message
        if hasattr(chunk, "message") and chunk.message is not None:
            if hasattr(chunk.message, "model") and chunk.message.model is not None:
                self._response_model = chunk.message.model

    def build_streaming_response(self, chunk):
        content = []
        # OpenAI
        if hasattr(chunk, "choices") and chunk.choices is not None:
            if not self.function_call and not self.tool_calls:
                for choice in chunk.choices:
                    if choice.delta and choice.delta.content is not None:
                        content = [choice.delta.content]
            elif self.function_call:
                for choice in chunk.choices:
                    if (
                        choice.delta
                        and choice.delta.function_call is not None
                        and choice.delta.function_call.arguments is not None
                    ):
                        content = [choice.delta.function_call.arguments]
            elif self.tool_calls:
                for choice in chunk.choices:
                    if choice.delta and choice.delta.tool_calls is not None:
                        toolcalls = choice.delta.tool_calls
                        content = []
                        for tool_call in toolcalls:
                            if (
                                tool_call
                                and tool_call.function is not None
                                and tool_call.function.arguments is not None
                            ):
                                content.append(tool_call.function.arguments)

        # VertexAI
        if hasattr(chunk, "text") and chunk.text is not None:
            content = [chunk.text]

        # Anthropic
        if hasattr(chunk, "delta") and chunk.delta is not None:
            content = [chunk.delta.text] if hasattr(chunk.delta, "text") else []

        if isinstance(chunk, dict):
            if "message" in chunk:
                if "content" in chunk["message"]:
                    content = [chunk["message"]["content"]]
        if content:
            self.result_content.append(content[0])

    def set_usage_attributes(self, chunk):

        # Anthropic & OpenAI
        if hasattr(chunk, "type") and chunk.type == "message_start":
            self.prompt_tokens = chunk.message.usage.input_tokens

        if hasattr(chunk, "usage") and chunk.usage is not None:
            if hasattr(chunk.usage, "output_tokens"):
                self.completion_tokens = chunk.usage.output_tokens

            if hasattr(chunk.usage, "prompt_tokens"):
                self.prompt_tokens = chunk.usage.prompt_tokens

            if hasattr(chunk.usage, "completion_tokens"):
                self.completion_tokens = chunk.usage.completion_tokens

        # VertexAI
        if hasattr(chunk, "usage_metadata"):
            self.completion_tokens = chunk.usage_metadata.candidates_token_count
            self.prompt_tokens = chunk.usage_metadata.prompt_token_count

        # Ollama
        if isinstance(chunk, dict):
            if "prompt_eval_count" in chunk:
                self.prompt_tokens = chunk["prompt_eval_count"]
            if "eval_count" in chunk:
                self.completion_tokens = chunk["eval_count"]

    def process_chunk(self, chunk):
        # Mistral nests the chunk data under a `data` attribute
        if (
            hasattr(chunk, "data")
            and chunk.data is not None
            and hasattr(chunk.data, "choices")
            and chunk.data.choices is not None
        ):
            chunk = chunk.data
        self.set_response_model(chunk=chunk)
        self.build_streaming_response(chunk=chunk)
        self.set_usage_attributes(chunk=chunk)


# From: langtrace_python_sdk/instrumentation/openai/types.py


class ContentItem:
    url: str
    revised_prompt: str
    base64: Optional[str]

    def __init__(
        self,
        url: str,
        revised_prompt: str,
        base64: Optional[str],
    ):
        self.url = url
        self.revised_prompt = revised_prompt
        self.base64 = base64


class ToolFunction:
    name: str
    arguments: str

    def __init__(
        self,
        name: str,
        arguments: str,
    ):
        self.name = name
        self.arguments = arguments


class ToolCall:
    id: str
    type: str
    function: ToolFunction

    def __init__(
        self,
        id: str,
        type: str,
        function: ToolFunction,
    ):
        self.id = id
        self.type = type
        self.function = function


class Message:
    role: str
    content: Union[str, List[ContentItem], Dict[str, Any]]
    tool_calls: Optional[List[ToolCall]]

    def __init__(
        self,
        role: str,
        content: Union[str, List[ContentItem], Dict[str, Any]],
        content_filter_results: Optional[Any],
    ):
        self.role = role
        self.content = content
        self.content_filter_results = content_filter_results


class Usage:
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

    def __init__(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
    ):
        self.prompt_tokens = prompt_tokens
        self.completion_tokens = completion_tokens
        self.total_tokens = total_tokens


class Choice:
    message: Message
    content_filter_results: Optional[Any]

    def __init__(
        self,
        message: Message,
        content_filter_results: Optional[Any],
    ):
        self.message = message
        self.content_filter_results = content_filter_results


class ImagesGenerateKwargs(TypedDict, total=False):
    operation_name: str
    model: Optional[str]
    messages: Optional[List[Message]]
    functions: Optional[List[ToolCall]]
    tools: Optional[List[ToolCall]]
    response_format: Optional[str]
    size: Optional[str]
    encoding_format: Optional[str]


class ImagesEditKwargs(TypedDict, total=False):
    response_format: Optional[str]
    size: Optional[str]


class ChatCompletionsCreateKwargs(TypedDict, total=False):
    model: Optional[str]
    messages: List[Message]
    functions: Optional[List[ToolCall]]
    tools: Optional[List[ToolCall]]


class EmbeddingsCreateKwargs(TypedDict, total=False):
    dimensions: Optional[str]
    input: Union[str, List[str], None]
    encoding_format: Optional[Union[List[str], str]]


class ResultType:
    model: Optional[str]
    content: List[ContentItem]
    system_fingerprint: Optional[str]
    usage: Optional[Usage]
    choices: Optional[List[Choice]]
    response_format: Optional[str]
    size: Optional[str]
    encoding_format: Optional[str]

    def __init__(
        self,
        model: Optional[str],
        role: Optional[str],
        content: List[ContentItem],
        system_fingerprint: Optional[str],
        usage: Optional[Usage],
        functions: Optional[List[ToolCall]],
        tools: Optional[List[ToolCall]],
        choices: Optional[List[Choice]],
        response_format: Optional[str],
        size: Optional[str],
        encoding_format: Optional[str],
    ):
        self.model = model
        self.role = role
        self.content = content
        self.system_fingerprint = system_fingerprint
        self.usage = usage
        self.functions = functions
        self.tools = tools
        self.choices = choices
        self.response_format = response_format
        self.size = size
        self.encoding_format = encoding_format
